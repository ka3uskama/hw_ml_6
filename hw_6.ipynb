{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 6: классификация текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом домашнем задании вам предстоит построить классификатор текстов!\n",
    "\n",
    "Данные мы будем использовать из Kaggle соревнования: https://www.kaggle.com/competitions/nlp-getting-started/data Оттуда надо скачать файл train.csv. На обучающую и тестовую выборки его поделим кодом ниже, менять его не надо!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем работать с датасетом постов из твиттера. Нам предстоит решать задачу бинарной классификации - определять содержатся ли в твитте информация о настоящей катастрофе/инциденте или нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train (2).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1186</th>\n",
       "      <td>1707</td>\n",
       "      <td>bridge%20collapse</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ashes 2015: AustraliaÛªs collapse at Trent Br...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4071</th>\n",
       "      <td>5789</td>\n",
       "      <td>hail</td>\n",
       "      <td>Carol Stream, Illinois</td>\n",
       "      <td>GREAT MICHIGAN TECHNIQUE CAMP\\nB1G THANKS TO @...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5461</th>\n",
       "      <td>7789</td>\n",
       "      <td>police</td>\n",
       "      <td>Houston</td>\n",
       "      <td>CNN: Tennessee movie theater shooting suspect ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5787</th>\n",
       "      <td>8257</td>\n",
       "      <td>rioting</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Still rioting in a couple of hours left until ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7445</th>\n",
       "      <td>10656</td>\n",
       "      <td>wounds</td>\n",
       "      <td>Lake Highlands</td>\n",
       "      <td>Crack in the path where I wiped out this morni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5226</th>\n",
       "      <td>7470</td>\n",
       "      <td>obliteration</td>\n",
       "      <td>Merica!</td>\n",
       "      <td>@Eganator2000 There aren't many Obliteration s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>7691</td>\n",
       "      <td>panic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>just had a panic attack bc I don't have enough...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>1242</td>\n",
       "      <td>blood</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Omron HEM-712C Automatic Blood Pressure Monito...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7603</th>\n",
       "      <td>10862</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Officials say a quarantine is in place at an A...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7270</th>\n",
       "      <td>10409</td>\n",
       "      <td>whirlwind</td>\n",
       "      <td>Stamford &amp; Cork (&amp; Shropshire)</td>\n",
       "      <td>I moved to England five years ago today. What ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5329 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id            keyword                        location  \\\n",
       "1186   1707  bridge%20collapse                             NaN   \n",
       "4071   5789               hail          Carol Stream, Illinois   \n",
       "5461   7789             police                        Houston    \n",
       "5787   8257            rioting                             NaN   \n",
       "7445  10656             wounds                  Lake Highlands   \n",
       "...     ...                ...                             ...   \n",
       "5226   7470       obliteration                         Merica!   \n",
       "5390   7691              panic                             NaN   \n",
       "860    1242              blood                             NaN   \n",
       "7603  10862                NaN                             NaN   \n",
       "7270  10409          whirlwind  Stamford & Cork (& Shropshire)   \n",
       "\n",
       "                                                   text  target  \n",
       "1186  Ashes 2015: AustraliaÛªs collapse at Trent Br...       0  \n",
       "4071  GREAT MICHIGAN TECHNIQUE CAMP\\nB1G THANKS TO @...       1  \n",
       "5461  CNN: Tennessee movie theater shooting suspect ...       1  \n",
       "5787  Still rioting in a couple of hours left until ...       1  \n",
       "7445  Crack in the path where I wiped out this morni...       0  \n",
       "...                                                 ...     ...  \n",
       "5226  @Eganator2000 There aren't many Obliteration s...       0  \n",
       "5390  just had a panic attack bc I don't have enough...       0  \n",
       "860   Omron HEM-712C Automatic Blood Pressure Monito...       0  \n",
       "7603  Officials say a quarantine is in place at an A...       1  \n",
       "7270  I moved to England five years ago today. What ...       1  \n",
       "\n",
       "[5329 rows x 5 columns]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1 (0.5 балла)\n",
    "\n",
    "Выведете на экран информацию о пропусках в данных. Если пропуски присутствуют заполните их пустой строкой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keyword</th>\n",
       "      <td>44</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <td>1760</td>\n",
       "      <td>773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Train  Test\n",
       "id            0     0\n",
       "keyword      44    17\n",
       "location   1760   773\n",
       "text          0     0\n",
       "target        0     0"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([train.isnull().sum(), test.isnull().sum()]).T.rename(columns = {0:'Train', 1:'Test'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.fillna('')\n",
    "test = test.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keyword</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Train  Test\n",
       "id            0     0\n",
       "keyword       0     0\n",
       "location      0     0\n",
       "text          0     0\n",
       "target        0     0"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([train.isnull().sum(), test.isnull().sum()]).T.rename(columns = {0:'Train', 1:'Test'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1186</th>\n",
       "      <td>1707</td>\n",
       "      <td>bridge%20collapse</td>\n",
       "      <td></td>\n",
       "      <td>Ashes 2015: AustraliaÛªs collapse at Trent Br...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4071</th>\n",
       "      <td>5789</td>\n",
       "      <td>hail</td>\n",
       "      <td>Carol Stream, Illinois</td>\n",
       "      <td>GREAT MICHIGAN TECHNIQUE CAMP\\nB1G THANKS TO @...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5461</th>\n",
       "      <td>7789</td>\n",
       "      <td>police</td>\n",
       "      <td>Houston</td>\n",
       "      <td>CNN: Tennessee movie theater shooting suspect ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5787</th>\n",
       "      <td>8257</td>\n",
       "      <td>rioting</td>\n",
       "      <td></td>\n",
       "      <td>Still rioting in a couple of hours left until ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7445</th>\n",
       "      <td>10656</td>\n",
       "      <td>wounds</td>\n",
       "      <td>Lake Highlands</td>\n",
       "      <td>Crack in the path where I wiped out this morni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5226</th>\n",
       "      <td>7470</td>\n",
       "      <td>obliteration</td>\n",
       "      <td>Merica!</td>\n",
       "      <td>@Eganator2000 There aren't many Obliteration s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>7691</td>\n",
       "      <td>panic</td>\n",
       "      <td></td>\n",
       "      <td>just had a panic attack bc I don't have enough...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>1242</td>\n",
       "      <td>blood</td>\n",
       "      <td></td>\n",
       "      <td>Omron HEM-712C Automatic Blood Pressure Monito...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7603</th>\n",
       "      <td>10862</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Officials say a quarantine is in place at an A...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7270</th>\n",
       "      <td>10409</td>\n",
       "      <td>whirlwind</td>\n",
       "      <td>Stamford &amp; Cork (&amp; Shropshire)</td>\n",
       "      <td>I moved to England five years ago today. What ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5329 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id            keyword                        location  \\\n",
       "1186   1707  bridge%20collapse                                   \n",
       "4071   5789               hail          Carol Stream, Illinois   \n",
       "5461   7789             police                        Houston    \n",
       "5787   8257            rioting                                   \n",
       "7445  10656             wounds                  Lake Highlands   \n",
       "...     ...                ...                             ...   \n",
       "5226   7470       obliteration                         Merica!   \n",
       "5390   7691              panic                                   \n",
       "860    1242              blood                                   \n",
       "7603  10862                                                      \n",
       "7270  10409          whirlwind  Stamford & Cork (& Shropshire)   \n",
       "\n",
       "                                                   text  target  \n",
       "1186  Ashes 2015: AustraliaÛªs collapse at Trent Br...       0  \n",
       "4071  GREAT MICHIGAN TECHNIQUE CAMP\\nB1G THANKS TO @...       1  \n",
       "5461  CNN: Tennessee movie theater shooting suspect ...       1  \n",
       "5787  Still rioting in a couple of hours left until ...       1  \n",
       "7445  Crack in the path where I wiped out this morni...       0  \n",
       "...                                                 ...     ...  \n",
       "5226  @Eganator2000 There aren't many Obliteration s...       0  \n",
       "5390  just had a panic attack bc I don't have enough...       0  \n",
       "860   Omron HEM-712C Automatic Blood Pressure Monito...       0  \n",
       "7603  Officials say a quarantine is in place at an A...       1  \n",
       "7270  I moved to England five years ago today. What ...       1  \n",
       "\n",
       "[5329 rows x 5 columns]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2 (1 балл)\n",
    "Давайте немного посмотрим на наши данные. Визуализируйте (где явно просят) или выведете информацию о следующем:\n",
    "\n",
    "1. Какое распределение классов в обучающей выборке?\n",
    "2. Посмотрите на колонку \"keyword\" - возьмите 10 наиболее встречающихся значений, постройте ступенчатую диаграмму распределения классов в зависимости от значения keyword, сделайте выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "К классу настоящих катастроф  относятся 2305 записей, 43.25% от всех записей\n",
      "К классу нерелевантных постов относятся 3024 записей, 56.75% от всех записей\n"
     ]
    }
   ],
   "source": [
    "temp = train.groupby('target').count()['id']\n",
    "print(f'К классу настоящих катастроф  относятся {temp[1]} записей, {round(temp[1] * 100/(temp[0]+temp[1]), 2)}% от всех записей')\n",
    "print(f'К классу нерелевантных постов относятся {temp[0]} записей, {round(temp[0] * 100/(temp[0]+temp[1]), 2)}% от всех записей')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0, 0, 'nuclear%20reactor'),\n",
       " Text(0, 0, 'damage'),\n",
       " Text(0, 0, 'siren'),\n",
       " Text(0, 0, 'emergency'),\n",
       " Text(0, 0, 'derail'),\n",
       " Text(0, 0, 'deluge'),\n",
       " Text(0, 0, 'fatality'),\n",
       " Text(0, 0, 'fatalities'),\n",
       " Text(0, 0, 'fear'),\n",
       " Text(0, 0, 'wreckage')]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "temp = train.groupby('keyword').count().sort_values('id', ascending=False)[1:11]\n",
    "data_0 = train[train['keyword'].isin(list(temp.index.values))]\n",
    "ax = sns.countplot(data=data_0, x='keyword', hue='target')\n",
    "txt = ax.get_xticklabels()\n",
    "ax.set_xticklabels(txt, rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 3 (0.5 балла) \n",
    "\n",
    "В этом задании предлагается объединить все три текстовых столбца в один (просто сконкатенировать cтроки) и убрать столбец с индексом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train['sentence'] = train['keyword'] + ' ' + train['location'] + ' ' + train['text']\n",
    "train = train.drop(['keyword', 'location', 'text', 'id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1186</th>\n",
       "      <td>0</td>\n",
       "      <td>bridge%20collapse  Ashes 2015: AustraliaÛªs c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4071</th>\n",
       "      <td>1</td>\n",
       "      <td>hail Carol Stream, Illinois GREAT MICHIGAN TEC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5461</th>\n",
       "      <td>1</td>\n",
       "      <td>police Houston  CNN: Tennessee movie theater s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5787</th>\n",
       "      <td>1</td>\n",
       "      <td>rioting  Still rioting in a couple of hours le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7445</th>\n",
       "      <td>0</td>\n",
       "      <td>wounds Lake Highlands Crack in the path where ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5226</th>\n",
       "      <td>0</td>\n",
       "      <td>obliteration Merica! @Eganator2000 There aren'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>0</td>\n",
       "      <td>panic  just had a panic attack bc I don't have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>0</td>\n",
       "      <td>blood  Omron HEM-712C Automatic Blood Pressure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7603</th>\n",
       "      <td>1</td>\n",
       "      <td>Officials say a quarantine is in place at an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7270</th>\n",
       "      <td>1</td>\n",
       "      <td>whirlwind Stamford &amp; Cork (&amp; Shropshire) I mov...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5329 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      target                                           sentence\n",
       "1186       0  bridge%20collapse  Ashes 2015: AustraliaÛªs c...\n",
       "4071       1  hail Carol Stream, Illinois GREAT MICHIGAN TEC...\n",
       "5461       1  police Houston  CNN: Tennessee movie theater s...\n",
       "5787       1  rioting  Still rioting in a couple of hours le...\n",
       "7445       0  wounds Lake Highlands Crack in the path where ...\n",
       "...      ...                                                ...\n",
       "5226       0  obliteration Merica! @Eganator2000 There aren'...\n",
       "5390       0  panic  just had a panic attack bc I don't have...\n",
       "860        0  blood  Omron HEM-712C Automatic Blood Pressure...\n",
       "7603       1    Officials say a quarantine is in place at an...\n",
       "7270       1  whirlwind Stamford & Cork (& Shropshire) I mov...\n",
       "\n",
       "[5329 rows x 2 columns]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 4 (0.5 балла)\n",
    "\n",
    "Далее мы будем пока работать только с train частью.\n",
    "\n",
    "1. Предобработайте данные (train часть) с помощью CountVectorizer.\n",
    "2. Какого размера получилась матрица?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5329, 18455)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from nltk.tokenize import word_tokenize\n",
    "#texts_tokenized = [' '.join([w for w in word_tokenize(t) if w.isalpha()]) for t in train['sentence']]\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(train['sentence']) # <--- text_tokenized\n",
    "X.shape\n",
    "\n",
    "# если убрать комменты все и поменять на text_tokenized в трансформе данные, то будет токенизация с word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 5 (1 балл)\n",
    "\n",
    "В предыдущем пункте у вас должна была получиться достаточно большая матрица.\n",
    "Если вы взгляните на текст, то увидете, что там есть множество специальных символов, ссылок и прочего мусора.\n",
    "\n",
    "Давайте также посмотрим на словарь, который получился в результате построения CountVectorizer, его можно найти в поле vocabulary_ инстанса этого класса. Давайте напишем функцию, которая печает ответы на следующие вопросы:\n",
    "\n",
    "1. Найдите в этом словаре все слова, которые содержат цифры. Сколько таких слов нашлось?\n",
    "\n",
    "2. Найдите все слова, которые содержат символы пунктуации. Сколько таких слов нашлось? \n",
    "\n",
    "3. Сколько хэштегов (токен начинается на #) и упоминаний (токен начинается на @) осталось в словаре?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cлов с числами: 3812\n",
      "Слов с знаками препинания: 315\n",
      "Слов с упоминаниями: 0\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "def check_numbers(dictionary):\n",
    "    numbers = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "    counter = 0\n",
    "    for word in dictionary:\n",
    "        for symbol in word:\n",
    "            if symbol in numbers:\n",
    "                counter+=1\n",
    "                break\n",
    "    return (counter)\n",
    "\n",
    "def check_punctuation(dictionary):\n",
    "    counter = 0\n",
    "    for word in dictionary:\n",
    "        for symbol in word:\n",
    "            if unicodedata.category(symbol).startswith(\"P\"):\n",
    "                counter+=1\n",
    "                break\n",
    "    return (counter)\n",
    "\n",
    "def check_mentions(dictionary):\n",
    "    counter = 0\n",
    "    for word in dictionary:\n",
    "        if (word[0] == '@' or word[0] == '#'):\n",
    "            counter+=1\n",
    "    return (counter)\n",
    "\n",
    "print(f'Cлов с числами: {check_numbers(cv.vocabulary_)}')\n",
    "print(f'Слов с знаками препинания: {check_punctuation(cv.vocabulary_)}')\n",
    "print(f'Слов с упоминаниями: {check_mentions(cv.vocabulary_)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 6 (0.5 балла)\n",
    "\n",
    "Вспомним, что на семинаре по текстам мы узнали, что в nltk есть специальный токенизатор для текстов - TweetTokenizer. Попробуем применить CountVectorizer с этим токенизатором. Ответьте на все вопросы из предыдущего пункта для TweetTokenizer и сравните результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5329, 11698)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "tw = TweetTokenizer()\n",
    "texts_tokenized = [' '.join([w for w in tw.tokenize(t) if w.isalpha()]) for t in train['sentence']]\n",
    "cv_2 = CountVectorizer()\n",
    "X = cv_2.fit_transform(texts_tokenized)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cлов с числами: 0\n",
      "Слов с знаками препинания: 0\n",
      "Слов с упоминаниями: 0\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "def check_numbers(dictionary):\n",
    "    numbers = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "    counter = 0\n",
    "    for word in dictionary:\n",
    "        for symbol in word:\n",
    "            if symbol in numbers:\n",
    "                counter+=1\n",
    "                break\n",
    "    return (counter)\n",
    "\n",
    "def check_punctuation(dictionary):\n",
    "    counter = 0\n",
    "    for word in dictionary:\n",
    "        for symbol in word:\n",
    "            if unicodedata.category(symbol).startswith(\"P\"):\n",
    "                counter+=1\n",
    "                break\n",
    "    return (counter)\n",
    "\n",
    "def check_mentions(dictionary):\n",
    "    counter = 0\n",
    "    for word in dictionary:\n",
    "        if (word[0] == '@' or word[0] == '#'):\n",
    "            counter+=1\n",
    "    return (counter)\n",
    "\n",
    "print(f'Cлов с числами: {check_numbers(cv_2.vocabulary_)}')\n",
    "print(f'Слов с знаками препинания: {check_punctuation(cv_2.vocabulary_)}')\n",
    "print(f'Слов с упоминаниями: {check_mentions(cv_2.vocabulary_)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 7 (2 балла)\n",
    "\n",
    "В scikit-learn мы можем оценивать процесс подсчета матрицы через CountVectorizer. У CountVectorizer, как и у других наследников \\_VectorizerMixin, есть аргумент tokenizer и preprocessor. preprocessor применится в самом начале к каждой строке вашего датасета, tokenizer же должен принять строку и вернуть токены.\n",
    "Давайте напишем кастомный токенайзер, которые сделает все, что нам нужно: \n",
    "\n",
    "0. Приведет все буквы к нижнему регистру\n",
    "1. Разобьет текст на токены с помощью TweetTokenizer из пакета nltk\n",
    "2. Удалит все токены содержащие не латинские буквы, кроме смайликов (будем считать ими токены содержащие только пунктуацию и, как минимум, одну скобочку) и хэштегов, которые после начальной # содержат только латинские буквы.\n",
    "3. Удалит все токены, которые перечислены в nltk.corpus.stopwords.words('english')\n",
    "4. Проведет стемминг с помощью SnowballStemmer\n",
    "\n",
    "Продемонстрируйте работу вашей функции на первых десяти текстах в обучающей выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "def latin_check(element):\n",
    "    if re.search(r'[^a-zA-Z]', element):\n",
    "        return (0)\n",
    "    return (1)\n",
    "\n",
    "def smile_check(element):\n",
    "    for i in element:\n",
    "        if i not in string.punctuation:\n",
    "            return (0)\n",
    "    for i in element:\n",
    "        if i == ')' or i == '(':\n",
    "            return (1)\n",
    "    return(0)\n",
    "\n",
    "def hashtag_check(element):\n",
    "    if element[0] == '#' and len(element) != 1:\n",
    "        if re.search(r'[^a-zA-Z]', element[1:]):\n",
    "            return (0)\n",
    "        return (1)\n",
    "    return (0)\n",
    "\n",
    "def check_rules(element):\n",
    "    if latin_check(element) == 0:\n",
    "        if smile_check(element):\n",
    "            return (1)\n",
    "        elif hashtag_check(element):\n",
    "            return (1)\n",
    "        else:\n",
    "            return (0)\n",
    "    elif element in nltk.corpus.stopwords.words('english'):\n",
    "        return (0)\n",
    "    else:\n",
    "        return (1)\n",
    "\n",
    "    \n",
    "def tokenize(string):\n",
    "    tt = TweetTokenizer()\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    temp = tt.tokenize(string.lower())\n",
    "    new = []\n",
    "    for i in temp:\n",
    "        if check_rules(i):\n",
    "            new.append(stemmer.stem(i))\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bridg', 'ash', 'australia', 'collaps', 'trent', 'bridg', 'among', 'worst', 'histori', 'england', 'bundl', 'australia']\n",
      "['hail', 'carol', 'stream', 'illinoi', 'great', 'michigan', 'techniqu', 'camp', 'thank', '#goblu', '#wrestleon']\n",
      "['polic', 'houston', 'cnn', 'tennesse', 'movi', 'theater', 'shoot', 'suspect', 'kill', 'polic']\n",
      "['riot', 'still', 'riot', 'coupl', 'hour', 'left', 'class']\n",
      "['wound', 'lake', 'highland', 'crack', 'path', 'wipe', 'morn', 'beach', 'run', 'surfac', 'wound', 'left', 'elbow', 'right', 'knee']\n",
      "['airplan', 'somewher', 'expert', 'franc', 'begin', 'examin', 'airplan', 'debri', 'found', 'reunion', 'island', 'french', 'air', 'accid', 'expert', '#mlb']\n",
      "['bloodi', 'isol', 'citi', 'world', 'perth', 'came', 'kill', 'indian', 'fun', 'video', 'smirk', 'remorseless', 'pakistani', 'killer', 'show', 'boast']\n",
      "['burn', 'except', 'idk', 'realli', 'burn']\n",
      "['destroy', '(', 'ask', ')', 'destroy', 'hous']\n",
      "['wound', 'maracay', 'nirgua', 'venezuela', 'polic', 'offic', 'wound', 'suspect', 'dead', 'exchang', 'shot']\n"
     ]
    }
   ],
   "source": [
    "for i in train['sentence'][0:10]:\n",
    "    print(tokenize(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 8 (1 балл)\n",
    "\n",
    "1. Примените CountVectorizer с реализованным выше токенизатором к обучающим и тестовым выборкам.\n",
    "2. Обучите LogisticRegression на полученных признаках.\n",
    "3. Посчитайте метрику f1-score на тестовых данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2644</th>\n",
       "      <td>1</td>\n",
       "      <td>destruction  So you have a new weapon that can...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2227</th>\n",
       "      <td>0</td>\n",
       "      <td>deluge  The f$&amp;amp;@ing things I do for #GISHW...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5448</th>\n",
       "      <td>1</td>\n",
       "      <td>police UK DT @georgegalloway: RT @Galloway4May...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0</td>\n",
       "      <td>aftershock  Aftershock back to school kick off...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6845</th>\n",
       "      <td>0</td>\n",
       "      <td>trauma Montgomery County, MD in response to tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4307</th>\n",
       "      <td>0</td>\n",
       "      <td>hellfire 570 Vanderbilt; Brooklyn, NY New cock...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3375</th>\n",
       "      <td>1</td>\n",
       "      <td>evacuation USA Bend Post Office roofers cut ga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710</th>\n",
       "      <td>1</td>\n",
       "      <td>collided  Monsoon flooding - Monsoon rains hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4898</th>\n",
       "      <td>1</td>\n",
       "      <td>massacre Ireland Remember this was a massacre ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6753</th>\n",
       "      <td>1</td>\n",
       "      <td>tornado Asheville, NC I liked a @YouTube video...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2284 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      target                                           sentence\n",
       "2644       1  destruction  So you have a new weapon that can...\n",
       "2227       0  deluge  The f$&amp;@ing things I do for #GISHW...\n",
       "5448       1  police UK DT @georgegalloway: RT @Galloway4May...\n",
       "132        0  aftershock  Aftershock back to school kick off...\n",
       "6845       0  trauma Montgomery County, MD in response to tr...\n",
       "...      ...                                                ...\n",
       "4307       0  hellfire 570 Vanderbilt; Brooklyn, NY New cock...\n",
       "3375       1  evacuation USA Bend Post Office roofers cut ga...\n",
       "1710       1  collided  Monsoon flooding - Monsoon rains hav...\n",
       "4898       1  massacre Ireland Remember this was a massacre ...\n",
       "6753       1  tornado Asheville, NC I liked a @YouTube video...\n",
       "\n",
       "[2284 rows x 2 columns]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['sentence'] = test['keyword'] + ' ' + test['location'] + ' ' + test['text']\n",
    "test = test.drop(['keyword', 'location', 'text', 'id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.linear_model import LogisticRegression # можно заменить на любимый классификатор\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(ngram_range=(1, 1))\n",
    "text_tokenized_train = [' '.join([w for w in tokenize(t) if w.isalpha()]) for t in train['sentence']]\n",
    "train['sentence'] = text_tokenized_train\n",
    "text_tokenized_test = [' '.join([w for w in tokenize(t) if w.isalpha()]) for t in test['sentence']]\n",
    "test['sentence'] = text_tokenized_test\n",
    "#bow = vec.fit_transform(train['sentence']) #обучающая часть в train датасете. Потом также буду сплитить и test.\n",
    "#bow_test = vec.transform(train['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer()\n",
    "bow = vec.fit_transform(train['sentence'])\n",
    "bow_test = vec.transform(test['sentence'])\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "bow = scaler.fit_transform(bow)\n",
    "bow_test = scaler.transform(bow_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.87      0.83      1318\n",
      "           1       0.79      0.68      0.73       966\n",
      "\n",
      "    accuracy                           0.79      2284\n",
      "   macro avg       0.79      0.77      0.78      2284\n",
      "weighted avg       0.79      0.79      0.79      2284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=200, random_state=42)\n",
    "clf.fit(bow, train['target'])\n",
    "pred = clf.predict(bow_test)\n",
    "print(classification_report(test['target'], pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 9 (1 балл)\n",
    "\n",
    "1. Повторите 7 задание, но с tf-idf векторизатором. Как изменилось качество?\n",
    "2. Мы можем еще сильнее уменьшить размер нашей матрицы, если отбросим значения df близкие к единице. Скорее всего такие слова не несут много информации о категории, так как встречаются достаточно часто. Ограничьте максимальный df в параметрах TfIdfVectorizer, поставьте верхнюю границу равную 0.9. Как изменился размер матрицы, как изменилось качество?\n",
    "3. Также мы можем уменьшить размер матрицы, удаляя слова со слишком маленьким df. Удалось ли добиться улучшения качества? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.86      0.83      1318\n",
      "           1       0.79      0.70      0.75       966\n",
      "\n",
      "    accuracy                           0.80      2284\n",
      "   macro avg       0.79      0.78      0.79      2284\n",
      "weighted avg       0.80      0.80      0.79      2284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vec = TfidfVectorizer(ngram_range=(1, 1))\n",
    "vec_train = vec.fit_transform(train['sentence'])\n",
    "vec_test = vec.transform(test['sentence'])\n",
    "\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "vec_train = scaler.fit_transform(vec_train)\n",
    "vec_test = scaler.transform(vec_test)\n",
    "\n",
    "clf = LogisticRegression(max_iter=300, random_state=42)\n",
    "clf.fit(vec_train, train['target'])\n",
    "pred_tfidf = clf.predict(vec_test)\n",
    "print(classification_report(test['target'], pred_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.86      0.83      1318\n",
      "           1       0.79      0.70      0.75       966\n",
      "\n",
      "    accuracy                           0.80      2284\n",
      "   macro avg       0.79      0.78      0.79      2284\n",
      "weighted avg       0.80      0.80      0.79      2284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vec = TfidfVectorizer(ngram_range=(1, 1), max_df = 0.9)\n",
    "vec_train = vec.fit_transform(train['sentence'])\n",
    "vec_test = vec.transform(test['sentence'])\n",
    "\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "vec_train = scaler.fit_transform(vec_train)\n",
    "vec_test = scaler.transform(vec_test)\n",
    "\n",
    "clf = LogisticRegression(max_iter=300, random_state=42)\n",
    "clf.fit(vec_train, train['target'])\n",
    "pred_tfidf = clf.predict(vec_test)\n",
    "print(classification_report(test['target'], pred_tfidf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.86      0.82      1318\n",
      "           1       0.78      0.69      0.74       966\n",
      "\n",
      "    accuracy                           0.79      2284\n",
      "   macro avg       0.79      0.78      0.78      2284\n",
      "weighted avg       0.79      0.79      0.79      2284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vec = TfidfVectorizer(ngram_range=(1, 1), max_df = 0.9, min_df = 0.002)\n",
    "vec_train = vec.fit_transform(train['sentence'])\n",
    "vec_test = vec.transform(test['sentence'])\n",
    "\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "vec_train = scaler.fit_transform(vec_train)\n",
    "vec_test = scaler.transform(vec_test)\n",
    "\n",
    "clf = LogisticRegression(max_iter=300, random_state=42)\n",
    "clf.fit(vec_train, train['target'])\n",
    "pred_tfidf = clf.predict(vec_test)\n",
    "print(classification_report(test['target'], pred_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Выводы:**\n",
    "\n",
    "1) Качество выросло!\n",
    "\n",
    "2) Изменения не обнаржены.\n",
    "\n",
    "3) Не удалось увеличить качество. При минимизации min_df оно стремится к показателям второго пункта."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 10 (1 балл)\n",
    "\n",
    "Еще один популяпный трюк, который позволит уменьшить количество признаков называется hashing trick. Его суть в том, то мы случайно группируем признаки ииии  ..... складываем их! А потом удаляем исходные признаки. В итоге все наши признаки это просто суммы исходных. Звучит странно, но это отлично работает. Давайте проверим этот трюк в нашем сеттинге.\n",
    "Также при таком подходе вам не нужно хранить словарь token->index, что тоже иногда полезно.\n",
    "\n",
    "1. Повторите задание 7 с HashingVectorizer, укажите количество фичей равное 5000.\n",
    "2. Какой из подходов показал самый высокий результат?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.85      0.82      1318\n",
      "           1       0.77      0.69      0.73       966\n",
      "\n",
      "    accuracy                           0.78      2284\n",
      "   macro avg       0.78      0.77      0.77      2284\n",
      "weighted avg       0.78      0.78      0.78      2284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "vec = HashingVectorizer(ngram_range=(1, 1), n_features = 5000)\n",
    "vec_train = vec.fit_transform(train['sentence'])\n",
    "vec_test = vec.transform(test['sentence'])\n",
    "\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "vec_train = scaler.fit_transform(vec_train)\n",
    "vec_test = scaler.transform(vec_test)\n",
    "\n",
    "clf = LogisticRegression(max_iter=300, random_state=42)\n",
    "clf.fit(vec_train, train['target'])\n",
    "pred_tfidf = clf.predict(vec_test)\n",
    "print(classification_report(test['target'], pred_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод:\n",
    "\n",
    "1) Этот крутой метод не позволил увеличить качество\n",
    "\n",
    "2) Самым крутым оказался старый добрый tfidefvectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 11 (1 балл)\n",
    "\n",
    "В этом задании нужно добиться f1 меры хотя в 0.75 на тестовых данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.86      0.83      1318\n",
      "           1       0.79      0.71      0.75       966\n",
      "\n",
      "    accuracy                           0.80      2284\n",
      "   macro avg       0.80      0.79      0.79      2284\n",
      "weighted avg       0.80      0.80      0.80      2284\n",
      "\n",
      "Финальное значение f1:0.7487791644058599\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "vec = TfidfVectorizer(ngram_range=(1, 2), max_df = 0.9, max_features=5000000, norm = 'l2', use_idf=True, sublinear_tf = True, analyzer = 'word', strip_accents='unicode', decode_error='strict')\n",
    "vec_train = vec.fit_transform(train['sentence'])\n",
    "vec_test = vec.transform(test['sentence'])\n",
    "\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "vec_train = scaler.fit_transform(vec_train)\n",
    "vec_test = scaler.transform(vec_test)\n",
    "\n",
    "clf = LogisticRegression(max_iter=200, random_state=13)\n",
    "clf.fit(vec_train, train['target'])\n",
    "pred_tfidf = clf.predict(vec_test)\n",
    "print(classification_report(test['target'], pred_tfidf))\n",
    "\n",
    "f1 = f1_score(test['target'], pred_tfidf)\n",
    "print(f'Финальное значение f1:{f1}')\n",
    "\n",
    "#админ я оч старался но на 1 сотую не могу никак емае:_( "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
